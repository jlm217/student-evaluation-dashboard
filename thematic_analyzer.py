#!/usr/bin/env python3
"""
Thematic Analysis Agent - Phase 2 (Methodologically-Aligned)

This script performs thematic synthesis on initial codes generated by the qualitative
coding agent. Enhanced with salience weighting, context-rich sampling, automated 
verification, and reflexive memos for transparent, defensible thematic analysis.
"""

import os
import json
import pandas as pd
import numpy as np
import random
import math
import google.generativeai as genai
from dotenv import load_dotenv
from typing import List, Dict, Any, Optional, Tuple
from collections import Counter

# Constants
DEFAULT_CODED_FILE = "SAMPLEComments_coded.csv"
MAX_VERIFICATION_RETRIES = 2

def load_config() -> bool:
    """
    Load environment variables and configure the Gemini client.
    
    Returns:
        bool: True if configuration is successful, False otherwise
    """
    try:
        # Load environment variables from .env file
        load_dotenv()
        
        # Get API key from environment
        api_key = os.getenv('GEMINI_API_KEY')
        if not api_key:
            print("Error: GEMINI_API_KEY not found in environment variables.")
            print("Please create a .env file with your API key:")
            print("GEMINI_API_KEY='your_api_key_here'")
            return False
        
        # Configure Gemini client
        genai.configure(api_key=api_key)
        print("‚úì Gemini API configured successfully")
        return True
        
    except Exception as e:
        print(f"Error loading configuration: {e}")
        return False

def load_coded_csv(file_path: str) -> Optional[pd.DataFrame]:
    """
    Load the coded CSV file and validate required columns.
    
    Args:
        file_path (str): Path to the coded CSV file
        
    Returns:
        pd.DataFrame or None: Loaded DataFrame or None if error
    """
    try:
        # Load CSV file
        df = pd.read_csv(file_path)
        print(f"‚úì Loaded coded CSV file: {file_path}")
        print(f"  Rows: {len(df)}, Columns: {len(df.columns)}")
        
        # Validate required columns
        required_cols = ['Initial_Code']
        missing_cols = [col for col in required_cols if col not in df.columns]
        
        if missing_cols:
            print(f"Error: Required columns missing: {missing_cols}")
            print(f"Available columns: {list(df.columns)}")
            return None
        
        # Remove rows with missing codes
        initial_rows = len(df)
        df = df.dropna(subset=['Initial_Code'])
        final_rows = len(df)
        
        if initial_rows > final_rows:
            print(f"  Removed {initial_rows - final_rows} rows with missing codes")
        
        print(f"‚úì Validated required columns")
        return df
        
    except Exception as e:
        print(f"Error loading coded CSV file: {e}")
        return None

def extract_unique_codes(df: pd.DataFrame) -> List[str]:
    """
    Extract unique initial codes from the DataFrame.
    LEGACY FUNCTION - Use compute_code_frequencies() for Phase 2 functionality.
    
    Args:
        df (pd.DataFrame): Coded DataFrame
        
    Returns:
        List[str]: List of unique initial codes
    """
    unique_codes = df['Initial_Code'].dropna().unique().tolist()
    print(f"‚úì Extracted {len(unique_codes)} unique initial codes")
    return unique_codes

def compute_code_frequencies(df: pd.DataFrame) -> Dict[str, int]:
    """
    Compute the frequency (count) of each unique Initial_Code for salience weighting.
    F1.1: The agent SHALL compute the frequency (count) of every unique Initial_Code.
    
    Args:
        df (pd.DataFrame): Coded DataFrame
        
    Returns:
        Dict[str, int]: Dictionary mapping codes to their frequencies
    """
    code_counts = df['Initial_Code'].dropna().value_counts().to_dict()
    total_codes = len(df['Initial_Code'].dropna())
    unique_codes = len(code_counts)
    
    print(f"‚úì Computed code frequencies: {unique_codes} unique codes from {total_codes} total responses")
    print(f"  Most frequent: {max(code_counts.items(), key=lambda x: x[1])} occurrences")
    print(f"  Frequency range: {min(code_counts.values())} - {max(code_counts.values())}")
    
    return code_counts

def get_random_sample(df: pd.DataFrame, code: str, target_sample_size: int, 
                     seed: int = 42) -> List[Dict[str, Any]]:
    """
    Get random sample of question-response pairs for a specific code with sentiment diversity.
    F2.1, F2.2, F2.3: Fetch all pairs, sample ‚àöN (min 1, max 10), preserve sentiment diversity.
    
    Args:
        df (pd.DataFrame): Full coded DataFrame
        code (str): The specific code to sample for
        target_sample_size (int): Target number of samples
        seed (int): Random seed for reproducibility
        
    Returns:
        List[Dict[str, Any]]: List of sampled question-response pairs with metadata
    """
    # Get all rows with this code
    code_rows = df[df['Initial_Code'] == code].copy()
    
    if code_rows.empty:
        return []
    
    # Set random seed for reproducibility
    random.seed(seed)
    np.random.seed(seed)
    
    # Ensure we have the required columns
    required_cols = ['question', 'response']
    if not all(col in code_rows.columns for col in required_cols):
        # Fall back to available columns
        available_cols = [col for col in required_cols if col in code_rows.columns]
        if not available_cols:
            return []
    
    # Cap sample size (min 1, max 10)
    sample_size = max(1, min(target_sample_size, 10, len(code_rows)))
    
    # Try to preserve sentiment diversity if Sentiment column exists
    if 'Sentiment' in code_rows.columns:
        sentiments = code_rows['Sentiment'].dropna().unique()
        samples = []
        
        # If we have both positive and negative, try to include at least one of each
        if len(sentiments) > 1 and 'Positive' in sentiments and 'Negative' in sentiments:
            # Get one positive and one negative if sample_size >= 2
            if sample_size >= 2:
                pos_samples = code_rows[code_rows['Sentiment'] == 'Positive']
                neg_samples = code_rows[code_rows['Sentiment'] == 'Negative']
                
                if not pos_samples.empty:
                    samples.append(pos_samples.sample(n=1, random_state=seed).iloc[0])
                if not neg_samples.empty:
                    samples.append(neg_samples.sample(n=1, random_state=seed).iloc[0])
                
                # Fill remaining slots randomly from all rows
                remaining_size = sample_size - len(samples)
                if remaining_size > 0:
                    remaining_rows = code_rows.drop([s.name for s in samples])
                    if not remaining_rows.empty:
                        additional = remaining_rows.sample(n=min(remaining_size, len(remaining_rows)), 
                                                         random_state=seed + 1)
                        samples.extend([row for _, row in additional.iterrows()])
            else:
                # Just one sample, pick randomly
                samples = [code_rows.sample(n=1, random_state=seed).iloc[0]]
        else:
            # No sentiment diversity or only one sentiment, sample randomly
            sampled_rows = code_rows.sample(n=sample_size, random_state=seed)
            samples = [row for _, row in sampled_rows.iterrows()]
    else:
        # No sentiment column, sample randomly
        sampled_rows = code_rows.sample(n=sample_size, random_state=seed)
        samples = [row for _, row in sampled_rows.iterrows()]
    
    # Convert to list of dictionaries with essential fields
    result = []
    for sample in samples[:sample_size]:  # Ensure we don't exceed target
        sample_dict = {
            'question': str(sample.get('question', 'N/A')),
            'response': str(sample.get('response', 'N/A')),
            'sentiment': str(sample.get('Sentiment', 'Unknown'))
        }
        # Include additional context if available
        for col in ['crs_number', 'SectionNumber_ASU', 'Instructor', 'Term']:
            if col in sample.index:
                sample_dict[col] = str(sample.get(col, 'N/A'))
        
        result.append(sample_dict)
    
    return result

def build_enriched_payload(df: pd.DataFrame, code_frequencies: Dict[str, int], 
                          seed: int = 42) -> List[Dict[str, Any]]:
    """
    Build enriched payload with code frequencies and random samples for each code.
    F2: Context-Rich Analysis with random sample excerpts.
    
    Args:
        df (pd.DataFrame): Full coded DataFrame
        code_frequencies (Dict[str, int]): Code frequency mapping
        seed (int): Random seed for reproducibility
        
    Returns:
        List[Dict[str, Any]]: Enriched payload with codes, counts, and samples
    """
    total_codes = len(code_frequencies)
    sample_size = max(1, min(5, int(math.sqrt(total_codes))))  # ‚åä‚àöN‚åã with max 5 samples per code
    
    print(f"‚úì Building enriched payload: {total_codes} codes, {sample_size} samples per code")
    
    enriched_codes = []
    
    for code, count in code_frequencies.items():
        # Get random samples for this code
        samples = get_random_sample(df, code, sample_size, seed)
        
        enriched_codes.append({
            'code': code,
            'count': count,
            'frequency_rank': None,  # Will be set after sorting
            'samples': samples
        })
    
    # Sort by count (most frequent first) and add frequency ranks
    enriched_codes.sort(key=lambda x: x['count'], reverse=True)
    for i, code_data in enumerate(enriched_codes):
        code_data['frequency_rank'] = i + 1
    
    return enriched_codes

def calculate_theme_target(num_unique_codes: int) -> int:
    """
    Calculate the target number of themes using a fixed range of 6-10 themes.
    This provides more consistent and manageable results than the square root formula.
    
    Args:
        num_unique_codes (int): Number of unique codes
        
    Returns:
        int: Target number of themes (6-10 range)
    """
    # Use a simple heuristic: fewer codes = fewer themes, more codes = more themes
    if num_unique_codes <= 20:
        target = 6
    elif num_unique_codes <= 40:
        target = 7
    elif num_unique_codes <= 60:
        target = 8
    elif num_unique_codes <= 80:
        target = 9
    else:
        target = 10
    
    print(f"‚úì Theme target calculated: {target} themes (from {num_unique_codes} codes, using 6-10 range)")
    return target

def verify_supporting_codes(themes_data: Dict[str, Any], 
                           valid_codes: set) -> Tuple[bool, List[str]]:
    """
    Verify that all supporting codes in themes exist in the original dataset.
    F3.1, F3.2: Detect codes cited but absent, and codes duplicated across themes.
    
    Args:
        themes_data (Dict[str, Any]): Themes data from LLM
        valid_codes (set): Set of valid codes from original dataset
        
    Returns:
        Tuple[bool, List[str]]: (is_valid, list_of_errors)
    """
    errors = []
    all_used_codes = []
    
    themes = themes_data.get('themes', [])
    
    for i, theme in enumerate(themes):
        theme_name = theme.get('theme_name', f'Theme {i+1}')
        supporting_codes = theme.get('supporting_codes', [])
        
        for code in supporting_codes:
            # Check if code exists in original dataset
            if code not in valid_codes:
                errors.append(f"‚ùå Theme '{theme_name}' cites non-existent code: '{code}'")
            
            # Track for duplication check
            all_used_codes.append((code, theme_name))
    
    # Check for duplicates across themes
    code_usage = {}
    for code, theme_name in all_used_codes:
        if code not in code_usage:
            code_usage[code] = []
        code_usage[code].append(theme_name)
    
    for code, theme_names in code_usage.items():
        if len(theme_names) > 1:
            themes_str = "', '".join(theme_names)
            errors.append(f"‚ùå Code '{code}' is duplicated across themes: '{themes_str}'")
    
    is_valid = len(errors) == 0
    
    if is_valid:
        print("‚úì Theme verification passed: All codes valid and unique")
    else:
        print(f"‚ùå Theme verification failed: {len(errors)} errors found")
        for error in errors:
            print(f"  {error}")
    
    return is_valid, errors

def create_thematic_prompt(codes_list: List[str]) -> str:
    """
    Create the thematic synthesis prompt with the provided codes.
    LEGACY FUNCTION - Use create_enhanced_thematic_prompt() for Phase 2 functionality.
    
    Args:
        codes_list (List[str]): List of initial codes to analyze
        
    Returns:
        str: Complete prompt for thematic analysis
    """
    prompt_template = """Role: You are a senior qualitative researcher specializing in grounded theory. You have been given a list of initial "open codes" derived from student feedback about a course.

Task: Your task is to perform a thematic analysis. You must analyze the provided list of initial codes to identify patterns, connections, and relationships. Group related codes together to synthesize a concise set of 7-10 overarching themes.

Instructions:

    Review Holistically: Analyze the entire list of codes to understand the full scope of student feedback.

    Identify Core Themes: Group the initial codes into distinct, high-level themes. A theme should represent a major concept or area of concern/praise.

    Name and Describe: For each theme, provide a clear and concise theme_name and a one-sentence theme_description that explains the central idea of that theme.

    Show Your Work: For each theme you create, list the exact initial codes from the provided data that you grouped together to form that theme. This is crucial for maintaining a clear audit trail.

    Format: Return your entire analysis as a single JSON object. The object should contain one key, "themes", which is a list. Each item in the list represents a theme and must have the keys: theme_name, theme_description, and supporting_codes (a list of strings).

Example of Desired Output Structure:

{
  "themes": [
    {
      "theme_name": "Instructor Engagement and Presence",
      "theme_description": "Concerns the students' perception of the instructor's involvement, accessibility, and feedback quality.",
      "supporting_codes": [
        "Lack of Instructor Feedback/Support",
        "Instructor not present in videos",
        "Slow response to student inquiries",
        "Positive instructor interaction"
      ]
    },
    {
      "theme_name": "Course Material and Platform Issues",
      "theme_description": "Feedback related to the effectiveness and accuracy of textbooks, online platforms, and other learning materials.",
      "supporting_codes": [
        "Smartbooks not adequate measure",
        "Inaccurate time estimates",
        "Helpful online resources"
      ]
    }
  ]
}

Data for Analysis:

Here is the list of initial codes generated from the student feedback. Please synthesize them into a set of themes.

""" + json.dumps(codes_list, indent=2) + """

Return your analysis as a JSON object:

```json"""
    
    return prompt_template

def create_enhanced_thematic_prompt(enriched_codes: List[Dict[str, Any]], 
                                  theme_target: int, correction_notes: List[str] = None) -> str:
    """
    Create enhanced thematic synthesis prompt with salience weighting and context samples.
    F1.2: LLM prompt MUST present counts and treat frequency as primary signal of prominence.
    F2: Include random sample excerpts for context-rich analysis.
    F4.1: Prompt MUST require memo field explaining grouping logic.
    
    Args:
        enriched_codes (List[Dict[str, Any]]): Enriched codes with frequencies and samples
        theme_target (int): Target number of themes (6-10 range)
        correction_notes (List[str], optional): Error corrections from verification
        
    Returns:
        str: Enhanced prompt for methodologically-aligned thematic analysis
    """
    # Build correction section if provided
    correction_section = ""
    if correction_notes:
        correction_section = f"""
**IMPORTANT CORRECTIONS NEEDED:**
The previous analysis had the following issues that must be fixed:
{chr(10).join(f'‚Ä¢ {note}' for note in correction_notes)}

Please carefully review these issues and ensure they are resolved in your new analysis.
"""
    
    prompt = f"""Role: You are a senior qualitative researcher with expertise in grounded theory and thematic analysis. Your task is to synthesize initial open codes from student feedback into a clear, well-structured set of actionable themes for use in educational program review.

{correction_section}

Input: You will be provided with a list of unique open codes. Each code object includes:
- A code label (summarizing a distinct student idea).
- Its count (the frequency this code appeared in the data).
- A small sample of representative excerpts from the original student comments.

Objective: Group the codes into a concise set of broad, descriptive themes based on patterns across the data. Each theme should represent a meaningful area of student feedback. The target number of themes should be approximately {theme_target} (optimized for manageability and clarity), but you should prioritize conceptual coherence over adhering to an exact number.

Guidelines & Instructions:

1. **Review Holistically:** Analyze the entire list of codes, their counts, and their excerpts to understand the full scope of student feedback.

2. **Grouping Logic:**
   - Group codes by conceptual similarity.
   - Give more weight to codes with a higher count when determining the importance and structure of themes.
   - Use the provided excerpts to understand the nuance and context behind each code.

3. **Theme Naming:**
   - Theme names must be short, neutral noun phrases (2‚Äì4 words max).
   - Do not use evaluative language (e.g., avoid "Excellent," "Frustrating," "Poor," "Negative," "Positive").
   - Good examples: "Instructor Communication," "Assessment Clarity," "Course Resources."

4. **Show Your Work:** For each theme, provide the exact list of supporting_codes that you grouped into it. Each code must appear in only one theme.

5. **Explain Grouping Decisions:** For each theme, provide a grouping_rationale that explains in 2-3 sentences why you grouped these specific codes together. Focus on the conceptual connections and patterns you identified.

6. **Provide a Memo:** After defining the themes, write a short Analyst Memo (max ~100 words) that explains your overall grouping strategy, any trade-offs you made, and what drove your key decisions.

Required Output Format:
Your entire response MUST be a single, valid JSON object.

{{
  "themes": [
    {{
      "theme_name": "Short Descriptive Name",
      "theme_description": "One sentence summarizing what this theme captures.",
      "supporting_codes": ["Exact code 1", "Exact code 2", "..."],
      "grouping_rationale": "Brief explanation (2-3 sentences) of why these specific codes were grouped together into this theme."
    }}
  ],
  "memo": "Short explanation of your grouping logic and key decisions (max ~100 words)."
}}

Example of Desired Output Structure:

{{
  "themes": [
    {{
      "theme_name": "Instructor Engagement",
      "theme_description": "Concerns students' perception of instructor involvement, accessibility, and feedback quality.",
      "supporting_codes": [
        "Lack of Instructor Feedback/Support",
        "Instructor not present in videos", 
        "Slow response to student inquiries",
        "Positive instructor interaction"
      ],
      "grouping_rationale": "These codes were grouped together because they all directly address the student's perception of the teaching staff's effectiveness and interpersonal skills. This includes codes related to teaching clarity, helpfulness, and the nature of their interactions, both positive and negative."
    }},
    {{
      "theme_name": "Course Materials",
      "theme_description": "Feedback related to the effectiveness and accuracy of textbooks, online platforms, and learning resources.",
      "supporting_codes": [
        "Smartbooks not adequate measure",
        "Inaccurate time estimates",
        "Helpful online resources"
      ],
      "grouping_rationale": "I identified a strong connection between codes about different learning resources and tools. While they relate to different artifacts, they all point to student concerns about the quality and usability of course materials and platforms."
    }}
  ],
  "memo": "I grouped codes by functional similarity, prioritizing high-frequency codes like instructor feedback patterns. I separated infrastructure issues (materials, platforms) from interpersonal dynamics (instructor engagement). Trade-offs included combining positive and negative codes about the same topic area to capture complete themes rather than sentiment-based groupings."
}}

Data for Analysis:

Here is the list of initial codes, their counts, and representative excerpts. Please synthesize them into approximately {theme_target} themes (aim for 6-10 themes total).

"""

    # Add each enriched code with its data
    for i, code_data in enumerate(enriched_codes, 1):
        code = code_data['code']
        count = code_data['count']
        rank = code_data['frequency_rank']
        samples = code_data['samples']
        
        prompt += f"\n**{rank}. '{code}' (count: {count})**\n"
        
        if samples:
            prompt += "Representative excerpts:\n"
            for j, sample in enumerate(samples, 1):
                question = sample['question'][:100] + "..." if len(sample['question']) > 100 else sample['question']
                response = sample['response']  # No truncation for responses
                sentiment = sample['sentiment']
                
                prompt += f"  {j}. Q: \"{question}\"\n"
                prompt += f"     A: \"{response}\" [{sentiment}]\n"
        else:
            prompt += "  (No samples available)\n"
        
        prompt += "\n"
    
    prompt += """
Return your analysis as a JSON object:

```json"""
    
    return prompt

def generate_themes_single_chunk(codes_list: List[str]) -> Optional[Dict[str, Any]]:
    """
    Generate themes from a single chunk of initial codes using Gemini API.
    
    Args:
        codes_list (List[str]): List of initial codes to analyze (should be <= 150 codes)
        
    Returns:
        Dict[str, Any] or None: Themes data or None if error
    """
    try:
        # Create the prompt
        prompt = create_thematic_prompt(codes_list)
        
        # Call Gemini API
        model = genai.GenerativeModel('gemini-2.5-flash')
        response = model.generate_content(prompt)
        
        # Parse JSON response
        response_text = response.text.strip()
        
        # Clean up response text (remove markdown code blocks if present)
        if response_text.startswith('```json'):
            response_text = response_text[7:]
        if response_text.endswith('```'):
            response_text = response_text[:-3]
        response_text = response_text.strip()
        
        # Parse JSON
        themes_data = json.loads(response_text)
        
        return themes_data
        
    except json.JSONDecodeError as e:
        print(f"Error parsing JSON response from chunk: {e}")
        return None
    except Exception as e:
        print(f"Error generating themes for chunk: {e}")
        return None

def generate_themes_enhanced(df: pd.DataFrame, enriched_codes: List[Dict[str, Any]], 
                           theme_target: int, valid_codes: set) -> Optional[Dict[str, Any]]:
    """
    Generate themes using enhanced methodology with verification and retry logic.
    F3: Automated verification and reliability with up to 2 retries.
    
    Args:
        df (pd.DataFrame): Original coded DataFrame  
        enriched_codes (List[Dict[str, Any]]): Enriched codes with frequencies and samples
        theme_target (int): Target number of themes
        valid_codes (set): Set of valid codes for verification
        
    Returns:
        Dict[str, Any] or None: Verified themes data with memo or None if error
    """
    retries_remaining = MAX_VERIFICATION_RETRIES
    correction_notes = None
    
    while retries_remaining >= 0:
        try:
            # Create enhanced prompt (with corrections if this is a retry)
            prompt = create_enhanced_thematic_prompt(enriched_codes, theme_target, correction_notes)
            
            print(f"üîÑ Generating themes with enhanced methodology (attempt {MAX_VERIFICATION_RETRIES - retries_remaining + 1})...")
            
            # Call Gemini API
            model = genai.GenerativeModel('gemini-2.5-flash')
            response = model.generate_content(prompt)
            
            # Parse JSON response
            response_text = response.text.strip()
            
            # Clean up response text (remove markdown code blocks if present)
            if response_text.startswith('```json'):
                response_text = response_text[7:]
            if response_text.endswith('```'):
                response_text = response_text[:-3]
            response_text = response_text.strip()
            
            # Parse JSON
            themes_data = json.loads(response_text)
            
            # Validate response structure
            if not isinstance(themes_data, dict):
                raise ValueError("Response must be a JSON object")
            
            if 'themes' not in themes_data:
                raise ValueError("Response must contain 'themes' field")
            
            if 'memo' not in themes_data:
                print("‚ö†Ô∏è  Warning: Response missing 'memo' field, adding default")
                themes_data['memo'] = "No analytical memo provided by the model."
            
            # Verify supporting codes
            is_valid, verification_errors = verify_supporting_codes(themes_data, valid_codes)
            
            if is_valid:
                print("‚úÖ Theme generation successful with verified codes")
                
                # Log memo for transparency
                memo = themes_data.get('memo', 'No memo provided')
                print(f"üß† AI Analytical Memo: {memo[:200]}{'...' if len(memo) > 200 else ''}")
                
                return themes_data
            else:
                # Verification failed
                if retries_remaining > 0:
                    print(f"üîÑ Verification failed, retrying with corrections ({retries_remaining} attempts remaining)")
                    correction_notes = verification_errors
                    retries_remaining -= 1
                    continue
                else:
                    print("‚ùå Max retries reached, returning themes with verification warnings")
                    # Add verification warnings to memo
                    verification_warning = f"\n\nVERIFICATION WARNINGS:\n" + "\n".join(verification_errors)
                    themes_data['memo'] = themes_data.get('memo', '') + verification_warning
                    return themes_data
            
        except json.JSONDecodeError as e:
            print(f"‚ùå JSON parsing error: {e}")
            if retries_remaining > 0:
                print(f"üîÑ Retrying due to JSON error ({retries_remaining} attempts remaining)")
                retries_remaining -= 1
                correction_notes = [f"Previous response had invalid JSON format: {str(e)}"]
                continue
            else:
                print("‚ùå Max retries reached due to JSON errors")
                return None
                
        except Exception as e:
            print(f"‚ùå Error in theme generation: {e}")
            if retries_remaining > 0:
                print(f"üîÑ Retrying due to generation error ({retries_remaining} attempts remaining)")
                retries_remaining -= 1
                correction_notes = [f"Previous attempt failed: {str(e)}"]
                continue
            else:
                print("‚ùå Max retries reached due to generation errors")
                return None
    
    return None


def create_consolidation_prompt(chunk_themes: List[Dict[str, Any]]) -> str:
    """
    Create a prompt for consolidating themes from multiple chunks.
    
    Args:
        chunk_themes (List[Dict]): List of theme objects from different chunks
        
    Returns:
        str: Consolidation prompt
    """
    prompt_template = """Role: You are a senior qualitative researcher performing meta-analysis on themes generated from multiple chunks of qualitative data.

Task: You have been given a list of themes that were generated from different chunks of student feedback codes. Your task is to consolidate these into a final set of 8-12 overarching themes by:

1. **Identifying Overlapping Themes**: Look for themes that represent the same core concept but may have slightly different names or descriptions.

2. **Merging Related Themes**: Combine themes that address the same domain or area of concern.

3. **Preserving Distinct Themes**: Keep themes that represent unique aspects of the feedback.

4. **Creating Comprehensive Themes**: Ensure your final themes capture the full breadth of the original data.

Instructions:
- Review all the themes provided below
- Group related/overlapping themes together
- Create a final set of consolidated themes with clear, descriptive names
- For each final theme, provide a comprehensive description and list ALL supporting codes from the original themes
- Return your analysis as a JSON object with the same structure as the input

Input Themes from Chunks:
"""
    
    # Add all chunk themes to the prompt
    for i, theme in enumerate(chunk_themes, 1):
        prompt_template += f"\n**Theme {i}:**\n"
        prompt_template += f"Name: {theme.get('theme_name', '')}\n"
        prompt_template += f"Description: {theme.get('theme_description', '')}\n"
        prompt_template += f"Supporting Codes: {', '.join(theme.get('supporting_codes', []))}\n"
    
    prompt_template += """\n\nReturn your consolidated analysis as a JSON object:

{
  "themes": [
    {
      "theme_name": "Consolidated Theme Name",
      "theme_description": "Comprehensive description that captures the essence of merged themes",
      "supporting_codes": ["all", "codes", "from", "merged", "themes"]
    }
  ]
}"""
    
    return prompt_template


def consolidate_themes(chunk_themes: List[Dict[str, Any]], max_themes_per_round: int = 20) -> Optional[Dict[str, Any]]:
    """
    Consolidate themes from multiple chunks into final themes using hierarchical approach.
    
    Args:
        chunk_themes (List[Dict]): List of theme objects from different chunks
        max_themes_per_round (int): Maximum themes to consolidate in one API call
        
    Returns:
        Dict[str, Any] or None: Consolidated themes data or None if error
    """
    try:
        print(f"Consolidating {len(chunk_themes)} themes from chunks...")
        
        # If we have few enough themes, consolidate them all at once
        if len(chunk_themes) <= max_themes_per_round:
            print("Consolidating all themes in single round...")
            prompt = create_consolidation_prompt(chunk_themes)
            
            model = genai.GenerativeModel('gemini-2.5-flash')
            response = model.generate_content(prompt)
            
            response_text = response.text.strip()
            if response_text.startswith('```json'):
                response_text = response_text[7:]
            if response_text.endswith('```'):
                response_text = response_text[:-3]
            response_text = response_text.strip()
            
            consolidated_themes = json.loads(response_text)
            print(f"‚úì Consolidated into {len(consolidated_themes.get('themes', []))} final themes")
            return consolidated_themes
        
        # Hierarchical consolidation for large sets
        print(f"Too many themes for single round, using hierarchical consolidation...")
        print(f"Processing themes in batches of {max_themes_per_round}...")
        
        current_themes = chunk_themes.copy()
        round_num = 1
        
        while len(current_themes) > max_themes_per_round:
            print(f"Consolidation round {round_num}: {len(current_themes)} themes -> batches of {max_themes_per_round}")
            
            # Split current themes into batches
            batches = [current_themes[i:i+max_themes_per_round] 
                      for i in range(0, len(current_themes), max_themes_per_round)]
            
            next_round_themes = []
            
            for i, batch in enumerate(batches, 1):
                print(f"  Processing batch {i}/{len(batches)} ({len(batch)} themes)...")
                
                try:
                    prompt = create_consolidation_prompt(batch)
                    model = genai.GenerativeModel('gemini-2.5-flash')
                    response = model.generate_content(prompt)
                    
                    response_text = response.text.strip()
                    if response_text.startswith('```json'):
                        response_text = response_text[7:]
                    if response_text.endswith('```'):
                        response_text = response_text[:-3]
                    response_text = response_text.strip()
                    
                    batch_result = json.loads(response_text)
                    if batch_result and 'themes' in batch_result:
                        batch_themes = batch_result['themes']
                        next_round_themes.extend(batch_themes)
                        print(f"    ‚úì Batch {i} consolidated to {len(batch_themes)} themes")
                    else:
                        print(f"    ‚ö†Ô∏è Batch {i} failed, keeping original themes")
                        next_round_themes.extend(batch)
                        
                except Exception as e:
                    print(f"    ‚ö†Ô∏è Error in batch {i}: {e}, keeping original themes")
                    next_round_themes.extend(batch)
            
            current_themes = next_round_themes
            round_num += 1
            print(f"Round {round_num-1} complete: {len(current_themes)} themes remaining")
            
            # Safety check to prevent infinite loops
            if round_num > 5:
                print("‚ö†Ô∏è Maximum consolidation rounds reached, stopping")
                break
        
        # Final consolidation round
        if len(current_themes) <= max_themes_per_round:
            print(f"Final consolidation round: {len(current_themes)} themes")
            prompt = create_consolidation_prompt(current_themes)
            
            model = genai.GenerativeModel('gemini-2.5-flash')
            response = model.generate_content(prompt)
            
            response_text = response.text.strip()
            if response_text.startswith('```json'):
                response_text = response_text[7:]
            if response_text.endswith('```'):
                response_text = response_text[:-3]
            response_text = response_text.strip()
            
            final_themes = json.loads(response_text)
            print(f"‚úì Final consolidation complete: {len(final_themes.get('themes', []))} themes")
            return final_themes
        else:
            # If still too many, return what we have
            print(f"‚ö†Ô∏è Still have {len(current_themes)} themes, returning as-is")
            return {'themes': current_themes}
        
    except json.JSONDecodeError as e:
        print(f"Error parsing JSON response from consolidation: {e}")
        return None
    except Exception as e:
        print(f"Error consolidating themes: {e}")
        return None


def generate_themes(codes_list: List[str], chunk_size: int = 150) -> Optional[Dict[str, Any]]:
    """
    Generate themes from the list of initial codes using chunked processing for large datasets.
    
    Args:
        codes_list (List[str]): List of initial codes to analyze
        chunk_size (int): Maximum number of codes per chunk (default: 150)
        
    Returns:
        Dict[str, Any] or None: Themes data or None if error
    """
    try:
        print(f"Generating themes from {len(codes_list)} initial codes...")
        
        # If the dataset is small enough, use the original single-call approach
        if len(codes_list) <= chunk_size:
            print("Dataset is small enough for single API call")
            return generate_themes_single_chunk(codes_list)
        
        # Phase 2A: Generate themes for each chunk
        print(f"Dataset is large, using chunked approach with {chunk_size} codes per chunk...")
        
        chunks = [codes_list[i:i+chunk_size] for i in range(0, len(codes_list), chunk_size)]
        print(f"Processing {len(chunks)} chunks...")
        
        all_chunk_themes = []
        
        for i, chunk in enumerate(chunks, 1):
            print(f"Processing chunk {i}/{len(chunks)} ({len(chunk)} codes)...")
            
            chunk_result = generate_themes_single_chunk(chunk)
            if chunk_result and 'themes' in chunk_result:
                chunk_themes = chunk_result['themes']
                all_chunk_themes.extend(chunk_themes)
                print(f"‚úì Generated {len(chunk_themes)} themes from chunk {i}")
            else:
                print(f"‚ö†Ô∏è Failed to generate themes for chunk {i}")
        
        if not all_chunk_themes:
            print("No themes generated from any chunks")
            return None
        
        print(f"Total themes from all chunks: {len(all_chunk_themes)}")
        
        # Phase 2B: Consolidate themes
        print("Consolidating themes across chunks...")
        consolidated_result = consolidate_themes(all_chunk_themes)
        
        if consolidated_result:
            final_themes = consolidated_result.get('themes', [])
            print(f"‚úì Successfully consolidated into {len(final_themes)} final themes")
            return consolidated_result
        else:
            # Fallback: return all chunk themes if consolidation fails
            print("‚ö†Ô∏è Consolidation failed, returning all chunk themes")
            return {'themes': all_chunk_themes}
        
    except Exception as e:
        print(f"Error in chunked theme generation: {e}")
        return None

def write_markdown_report(themes_data: Dict[str, Any], output_path: str = "themes_report.md") -> bool:
    """
    Write a markdown report of the themes analysis.
    
    Args:
        themes_data (Dict[str, Any]): Themes data from API
        output_path (str): Path for the markdown report
        
    Returns:
        bool: True if successful, False otherwise
    """
    try:
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write("# Thematic Analysis Report\n\n")
            f.write("## Overview\n\n")
            
            themes = themes_data.get('themes', [])
            f.write(f"This report presents the results of thematic analysis performed on qualitative codes. ")
            f.write(f"A total of {len(themes)} themes were identified through axial coding.\n\n")
            
            f.write("## Themes\n\n")
            
            for i, theme in enumerate(themes, 1):
                theme_name = theme.get('theme_name', f'Theme {i}')
                theme_description = theme.get('theme_description', 'No description provided')
                supporting_codes = theme.get('supporting_codes', [])
                
                f.write(f"### {i}. {theme_name}\n\n")
                f.write(f"**Description:** {theme_description}\n\n")
                f.write(f"**Supporting Codes ({len(supporting_codes)}):**\n\n")
                
                for code in supporting_codes:
                    f.write(f"- {code}\n")
                
                f.write("\n---\n\n")
        
        print(f"‚úì Markdown report saved to: {output_path}")
        return True
        
    except Exception as e:
        print(f"Error writing markdown report: {e}")
        return False

def write_csv_summary(themes_data: Dict[str, Any], output_path: str = "themes_summary.csv") -> bool:
    """
    Write a CSV summary of themes with names and descriptions.
    
    Args:
        themes_data (Dict[str, Any]): Themes data from API
        output_path (str): Path for the CSV summary
        
    Returns:
        bool: True if successful, False otherwise
    """
    try:
        themes = themes_data.get('themes', [])
        
        # Create summary DataFrame
        summary_data = []
        for theme in themes:
            summary_data.append({
                'Theme_Name': theme.get('theme_name', ''),
                'Theme_Description': theme.get('theme_description', ''),
                'Supporting_Codes_Count': len(theme.get('supporting_codes', []))
            })
        
        summary_df = pd.DataFrame(summary_data)
        summary_df.to_csv(output_path, index=False)
        
        print(f"‚úì CSV summary saved to: {output_path}")
        return True
        
    except Exception as e:
        print(f"Error writing CSV summary: {e}")
        return False

def create_themed_csv(original_df: pd.DataFrame, themes_data: Dict[str, Any], input_file_path: str) -> bool:
    """
    Create a CSV file with theme assignments for each row.
    
    Args:
        original_df (pd.DataFrame): Original coded DataFrame
        themes_data (Dict[str, Any]): Themes data from API
        input_file_path (str): Path to input file for naming output
        
    Returns:
        bool: True if successful, False otherwise
    """
    try:
        # Create code-to-theme mapping
        code_to_theme = {}
        themes = themes_data.get('themes', [])
        
        for theme in themes:
            theme_name = theme.get('theme_name', '')
            supporting_codes = theme.get('supporting_codes', [])
            
            for code in supporting_codes:
                code_to_theme[code] = theme_name
        
        # Create themed DataFrame
        themed_df = original_df.copy()
        themed_df['Theme'] = themed_df['Initial_Code'].map(code_to_theme)
        
        # Handle unmapped codes
        unmapped_count = themed_df['Theme'].isna().sum()
        if unmapped_count > 0:
            print(f"Warning: {unmapped_count} codes could not be mapped to themes")
            themed_df['Theme'] = themed_df['Theme'].fillna('Unmapped')
        
        # Aggressively clean text fields to prevent CSV parsing issues
        text_columns = ['question', 'response', 'Initial_Code', 'Theme']
        for col in text_columns:
            if col in themed_df.columns:
                # Convert to string first
                themed_df[col] = themed_df[col].astype(str)
                # Remove ALL types of line breaks and control characters
                themed_df[col] = themed_df[col].str.replace('\n', ' ', regex=False)
                themed_df[col] = themed_df[col].str.replace('\r', ' ', regex=False)
                themed_df[col] = themed_df[col].str.replace('\t', ' ', regex=False)
                # Replace double quotes with single quotes to prevent quote conflicts
                themed_df[col] = themed_df[col].str.replace('"', "'", regex=False)
                # Remove any control characters that could break CSV parsing
                themed_df[col] = themed_df[col].str.replace(r'[\x00-\x1f\x7f-\x9f]', ' ', regex=True)
                # Consolidate multiple spaces to single space
                themed_df[col] = themed_df[col].str.replace(r'\s+', ' ', regex=True)
                # Strip leading/trailing whitespace
                themed_df[col] = themed_df[col].str.strip()
                # Cap extremely long text that could cause issues
                themed_df[col] = themed_df[col].str.slice(0, 8000)
        
        # Generate output filename
        base_name = os.path.splitext(input_file_path)[0]
        output_path = f"{base_name}_themed.csv"
        
        # Save CSV with the most robust settings possible
        themed_df.to_csv(
            output_path, 
            index=False, 
            quoting=1,              # QUOTE_ALL - quote every single field
            doublequote=True,       # Handle any remaining quotes correctly
            lineterminator='\n',    # Consistent line endings
            encoding='utf-8',       # Proper encoding
            escapechar=None         # Don't use escape characters
        )
        
        print(f"‚úì Themed CSV saved to: {output_path}")
        return True
        
    except Exception as e:
        print(f"Error creating themed CSV: {e}")
        return False

def create_themed_dataframe(original_df: pd.DataFrame, themes_data: Dict[str, Any]) -> pd.DataFrame:
    """
    Create a DataFrame with theme assignments for each row.
    
    Args:
        original_df (pd.DataFrame): Original coded DataFrame
        themes_data (Dict[str, Any]): Themes data from API
        
    Returns:
        pd.DataFrame: DataFrame with Theme column added
    """
    # Create code-to-theme mapping
    code_to_theme = {}
    themes = themes_data.get('themes', [])
    
    for theme in themes:
        theme_name = theme.get('theme_name', '')
        supporting_codes = theme.get('supporting_codes', [])
        
        for code in supporting_codes:
            code_to_theme[code] = theme_name
    
    # Create themed DataFrame
    themed_df = original_df.copy()
    themed_df['Theme'] = themed_df['Initial_Code'].map(code_to_theme)
    
    # Handle unmapped codes
    unmapped_count = themed_df['Theme'].isna().sum()
    if unmapped_count > 0:
        print(f"Warning: {unmapped_count} codes could not be mapped to themes")
        themed_df['Theme'] = themed_df['Theme'].fillna('Unmapped')
    
    # Clean text fields for better JSON serialization
    text_columns = ['question', 'response', 'Initial_Code', 'Theme']
    for col in text_columns:
        if col in themed_df.columns:
            # Convert to string and clean
            themed_df[col] = themed_df[col].astype(str)
            themed_df[col] = themed_df[col].str.replace('\n', ' ', regex=False)
            themed_df[col] = themed_df[col].str.replace('\r', ' ', regex=False)
            themed_df[col] = themed_df[col].str.replace('\t', ' ', regex=False)
            themed_df[col] = themed_df[col].str.replace(r'\s+', ' ', regex=True)
            themed_df[col] = themed_df[col].str.strip()
            # Cap extremely long text
            themed_df[col] = themed_df[col].str.slice(0, 8000)
    
    return themed_df


def generate_markdown_report(themes_data: Dict[str, Any]) -> str:
    """
    Generate a markdown report string from themes data.
    LEGACY FUNCTION - Use generate_enhanced_markdown_report() for Phase 2 functionality.
    
    Args:
        themes_data (Dict[str, Any]): Themes data from API
        
    Returns:
        str: Markdown report as string
    """
    report_lines = []
    report_lines.append("# Thematic Analysis Report\n")
    report_lines.append("## Overview\n")
    
    themes = themes_data.get('themes', [])
    report_lines.append(f"This report presents the results of thematic analysis performed on qualitative codes. ")
    report_lines.append(f"A total of {len(themes)} themes were identified through axial coding.\n")
    
    report_lines.append("## Themes\n")
    
    for i, theme in enumerate(themes, 1):
        theme_name = theme.get('theme_name', f'Theme {i}')
        theme_description = theme.get('theme_description', 'No description provided')
        supporting_codes = theme.get('supporting_codes', [])
        
        report_lines.append(f"### {i}. {theme_name}\n")
        report_lines.append(f"**Description:** {theme_description}\n")
        report_lines.append(f"**Supporting Codes ({len(supporting_codes)}):**\n")
        
        for code in supporting_codes:
            report_lines.append(f"- {code}")
        
        report_lines.append("\n---\n")
    
    return '\n'.join(report_lines)

def generate_ai_analysis_memo(themes_data: Dict[str, Any], job_id: str = None) -> str:
    """
    Generate AI Analysis Memo with grouping rationales for each theme.
    
    Args:
        themes_data (Dict[str, Any]): Themes data with grouping rationales
        job_id (str, optional): Job ID for the analysis
        
    Returns:
        str: Formatted AI Analysis Memo
    """
    themes = themes_data.get('themes', [])
    
    if not themes:
        return "No thematic analysis available for grouping rationale memo."
    
    # Header
    memo_lines = ["# AI Analysis Memo: Grouping Rationale for Thematic Analysis\n"]
    
    if job_id:
        memo_lines.append(f"**Analysis ID:** {job_id}\n")
    
    memo_lines.append("This document provides transparency into the key analytical decisions made by the AI during the thematic analysis phase. For each theme generated, the AI provided a brief memo explaining its reasoning for grouping a specific set of initial codes together.\n")
    
    # Theme rationales
    for i, theme in enumerate(themes, 1):
        theme_name = theme.get('theme_name', f'Theme {i}')
        grouping_rationale = theme.get('grouping_rationale', 'No rationale provided.')
        
        memo_lines.append(f"## Theme {i}: {theme_name}\n")
        memo_lines.append(f"**AI Grouping Rationale:** \"{grouping_rationale}\"\n")
    
    return '\n'.join(memo_lines)

def generate_enhanced_markdown_report(themes_data: Dict[str, Any], 
                                    code_frequencies: Dict[str, int], 
                                    theme_target: int) -> str:
    """
    Generate enhanced markdown report with methodological details and AI memo.
    F4.2: Memo SHALL flow through pipeline and be exposed in front-end.
    
    Args:
        themes_data (Dict[str, Any]): Themes data from enhanced analysis
        code_frequencies (Dict[str, int]): Original code frequency data
        theme_target (int): Target number of themes used
        
    Returns:
        str: Enhanced markdown report with methodological transparency
    """
    report_lines = []
    
    # Header and methodology
    report_lines.append("# Methodologically-Aligned Thematic Analysis Report\n")
    report_lines.append("## Methodology Overview\n")
    
    themes = themes_data.get('themes', [])
    total_responses = sum(code_frequencies.values())
    num_unique_codes = len(code_frequencies)
    
    report_lines.append(f"This report presents results from a methodologically-aligned thematic analysis using:")
    report_lines.append(f"- **Salience Weighting**: {num_unique_codes} initial codes weighted by frequency ({total_responses} total responses)")
    report_lines.append(f"- **Context Sampling**: Random samples with sentiment diversity for each code")
    report_lines.append(f"- **Theme Target**: {theme_target} themes (6-10 range optimized for clarity)")
    report_lines.append(f"- **Verification**: Automated code validation with retry logic")
    report_lines.append(f"- **Transparency**: AI analytical memo documenting reasoning\n")
    
    # AI Analytical Memo
    memo = themes_data.get('memo', 'No analytical memo provided')
    if memo:
        report_lines.append("## AI Analytical Memo\n")
        report_lines.append("*The following memo documents the AI's analytical reasoning and methodological decisions:*\n")
        report_lines.append(f"{memo}\n")
        report_lines.append("---\n")
    
    # Themes with enhanced details
    report_lines.append("## Thematic Results\n")
    report_lines.append(f"**Summary**: {len(themes)} themes generated from {num_unique_codes} initial codes.\n")
    
    for i, theme in enumerate(themes, 1):
        theme_name = theme.get('theme_name', f'Theme {i}')
        theme_description = theme.get('theme_description', 'No description provided')
        supporting_codes = theme.get('supporting_codes', [])
        
        # Calculate theme frequency statistics
        theme_frequency = sum(code_frequencies.get(code, 0) for code in supporting_codes)
        frequency_percentage = (theme_frequency / total_responses) * 100 if total_responses > 0 else 0
        
        report_lines.append(f"### {i}. {theme_name}\n")
        report_lines.append(f"**Description:** {theme_description}\n")
        report_lines.append(f"**Salience:** {theme_frequency} responses ({frequency_percentage:.1f}% of total)\n")
        report_lines.append(f"**Supporting Codes ({len(supporting_codes)}):**\n")
        
        # Sort codes by frequency within theme
        code_freq_pairs = [(code, code_frequencies.get(code, 0)) for code in supporting_codes]
        code_freq_pairs.sort(key=lambda x: x[1], reverse=True)
        
        for code, freq in code_freq_pairs:
            report_lines.append(f"- {code} *(n={freq})*")
        
        report_lines.append("\n---\n")
    
    # Methodological appendix
    report_lines.append("## Methodological Notes\n")
    report_lines.append("- **Salience Weighting**: Higher frequency codes weighted as more salient in thematic decisions")
    report_lines.append("- **Context Preservation**: Original student voices sampled to prevent misinterpretation")
    report_lines.append("- **Verification**: All codes verified for accuracy and mutual exclusivity across themes")
    report_lines.append("- **Transparency**: AI reasoning documented for methodological accountability\n")
    
    return '\n'.join(report_lines)


def analyze_themes(coded_df: pd.DataFrame, input_file_path: str = None, job_id: str = None) -> tuple[pd.DataFrame, str, dict]:
    """
    Perform thematic analysis on coded qualitative data using two-pass methodology.
    Phase 2: Two-Pass Thematic Analysis Agent with salience weighting, context samples,
    theme generation pass, and comprehensive code assignment pass.
    
    Args:
        coded_df (pd.DataFrame): DataFrame with coded data
        input_file_path (str, optional): Original file path for naming outputs (backward compatibility)
        job_id (str, optional): Job ID for the analysis (used in AI analysis memo)
        
    Returns:
        tuple[pd.DataFrame, str, dict]: (themed_dataframe, markdown_report, themes_data)
    """
    print("=== Thematic Analysis Agent - Phase 2 (Two-Pass Strategy) ===\n")
    
    # Load configuration
    if not load_config():
        raise Exception("Failed to load API configuration")
    
    # Validate the DataFrame
    if coded_df is None or coded_df.empty:
        raise ValueError("Input DataFrame is empty or None")
    
    if 'Initial_Code' not in coded_df.columns:
        raise ValueError("DataFrame must contain 'Initial_Code' column")
    
    # Remove rows with missing codes
    initial_rows = len(coded_df)
    df = coded_df.dropna(subset=['Initial_Code']).copy()
    final_rows = len(df)
    
    if initial_rows > final_rows:
        print(f"Removed {initial_rows - final_rows} rows with missing codes")
    
    # F1.1: Compute code frequencies for salience weighting
    print("üìä Computing code frequencies for salience analysis...")
    code_frequencies = compute_code_frequencies(df)
    
    if not code_frequencies:
        print("No codes found for thematic analysis")
        # Return original DataFrame with empty Theme column and basic report
        df['Theme'] = 'No themes generated'
        report = "# Thematic Analysis Report\n\nNo codes found for analysis."
        return df, report, {}
    
    # F1.3: Calculate theme target using 6-10 range
    num_unique_codes = len(code_frequencies)
    theme_target = calculate_theme_target(num_unique_codes)
    
    # F2: Build enriched payload with context samples
    print("üîç Building enriched payload with context samples...")
    enriched_codes = build_enriched_payload(df, code_frequencies, seed=42)
    
    # Create set of valid codes for verification
    valid_codes = set(code_frequencies.keys())
    
    # F3: Generate themes using two-pass strategy
    print(f"üéØ Starting two-pass thematic synthesis (target: {theme_target} themes)...")
    themes_data = generate_themes_two_pass(df, enriched_codes, theme_target, valid_codes)
    
    if themes_data is None:
        print("Failed to generate themes with two-pass methodology")
        df['Theme'] = 'Theme generation failed'
        report = "# Thematic Analysis Report\n\nTheme generation failed."
        return df, report, {}
    
    # Create themed DataFrame
    themed_df = create_themed_dataframe(df, themes_data)
    
    # Generate enhanced markdown report with memo
    markdown_report = generate_enhanced_markdown_report(themes_data, code_frequencies, theme_target)
    
    # Generate AI analysis memo for grouping rationales
    ai_analysis_memo = generate_ai_analysis_memo(themes_data, job_id)
    themes_data['ai_analysis_memo'] = ai_analysis_memo
    
    print(f"=== Two-Pass Thematic Analysis Complete ===")
    print(f"Total rows processed: {len(themed_df)}")
    print(f"Unique codes analyzed: {num_unique_codes}")
    print(f"Target themes: {theme_target}")
    print(f"Themes generated: {len(themes_data.get('themes', []))}")
    
    # Show generated themes with frequency data
    themes = themes_data.get('themes', [])
    if themes:
        print(f"Generated Themes:")
        for i, theme in enumerate(themes, 1):
            theme_name = theme.get('theme_name', f'Theme {i}')
            supporting_codes = theme.get('supporting_codes', [])
            supporting_count = len(supporting_codes)
            
            # Calculate total frequency weight for this theme
            theme_frequency = sum(code_frequencies.get(code, 0) for code in supporting_codes)
            frequency_percentage = (theme_frequency / sum(code_frequencies.values())) * 100
            
            print(f"  {i}. {theme_name} ({supporting_count} codes, {theme_frequency} responses, {frequency_percentage:.1f}%)")
    
    # Show AI memo for transparency
    memo = themes_data.get('memo', 'No memo provided')
    if memo:
        print(f"\nüß† AI Analytical Memo:")
        print(f"   {memo[:300]}{'...' if len(memo) > 300 else ''}")
    
    # Save outputs if file path provided (for backward compatibility)
    if input_file_path:
        write_markdown_report(themes_data, "themes_report.md")
        write_csv_summary(themes_data, "themes_summary.csv")
        create_themed_csv(themed_df, themes_data, input_file_path)
    
    return themed_df, markdown_report, themes_data


def analyze_themes_legacy(input_file: str) -> List[str]:
    """
    Legacy function for backward compatibility.
    
    Args:
        input_file (str): Path to the coded CSV file
        
    Returns:
        List[str]: List of output file paths generated
    """
    output_files = []
    
    if not os.path.exists(input_file):
        raise FileNotFoundError(f"File '{input_file}' not found")
    
    # Load coded CSV data
    df = load_coded_csv(input_file)
    if df is None:
        raise Exception("Failed to load coded CSV data")
    
    try:
        themed_df, markdown_report, themes_data = analyze_themes(df, input_file)
        
        # Files were saved by analyze_themes since we provided input_file_path
        output_files.append("themes_report.md")
        output_files.append("themes_summary.csv")
        
        base_name = os.path.splitext(input_file)[0]
        themed_path = f"{base_name}_themed.csv"
        output_files.append(themed_path)
        
        return output_files
        
    except Exception as e:
        print(f"Legacy analysis failed: {e}")
        raise


def main():
    """
    Main function to orchestrate the thematic analysis process.
    """
    print("=== Thematic Analysis Agent - Phase 2 (Two-Pass Strategy) ===\n")
    
    # Load configuration
    if not load_config():
        return
    
    # Get input file path from user
    input_file = input(f"Enter the path to your coded CSV file (default: '{DEFAULT_CODED_FILE}'): ").strip()
    if not input_file:
        input_file = DEFAULT_CODED_FILE
        
    if not os.path.exists(input_file):
        print(f"Error: File '{input_file}' not found.")
        return
    
    try:
        analyze_themes_legacy(input_file)
    except Exception as e:
        print(f"Error: {e}")

if __name__ == "__main__":
    main()

# Add two-pass strategy functions after the existing functions

def create_theme_generation_prompt(enriched_codes: List[Dict[str, Any]], theme_target: int) -> str:
    """
    Create the first pass prompt for theme generation only.
    
    Args:
        enriched_codes (List[Dict[str, Any]]): Enriched codes with frequencies and samples
        theme_target (int): Target number of themes
        
    Returns:
        str: Theme generation prompt
    """
    prompt = f"""Role: You are a senior qualitative researcher. Your task is to analyze a list of open codes from student feedback and define a set of high-level themes.

Input: You will be provided with a list of unique open codes, their frequency counts, and representative excerpts.

Objective: Review all the codes and identify the primary conceptual pillars in the data. Based on your analysis, define approximately **{theme_target}** distinct themes. Do NOT assign codes to themes in this step. Your only goal is to define the "buckets."

Instructions:
1. **Analyze Holistically:** Review the entire list of codes, counts, and excerpts.
2. **Define Themes:** For each conceptual pillar you identify, provide a `theme_name` and a `theme_description`.
3. **Theme Naming:** Theme names must be short, neutral noun phrases (e.g., "Instructor Communication," "Assessment Clarity").
4. **Provide a Memo:** After defining the themes, write a short Analyst Memo explaining your overall strategy for defining these themes.

Required Output Format:
Your entire response MUST be a single, valid JSON object.

{{
  "themes": [
    {{
      "theme_name": "Short Descriptive Name",
      "theme_description": "One sentence summarizing what this theme captures."
    }}
  ],
  "memo": "Short explanation of your high-level grouping logic (max ~100 words)."
}}

Data for Analysis:
Here is the list of initial codes, their counts, and representative excerpts. Please define approximately {theme_target} themes based on this data.

"""
    
    # Add enriched codes data
    for code_info in enriched_codes:
        code = code_info['code']
        count = code_info['count']
        samples = code_info['samples']
        
        prompt += f"\n**Code:** {code} (frequency: {count})\n"
        
        # Add sample excerpts
        if samples:
            prompt += "Sample excerpts:\n"
            for sample in samples[:2]:  # Show first 2 samples
                question = sample.get('question', 'N/A')
                response = sample.get('response', 'N/A')
                prompt += f"- Q: {question[:100]}... A: {response[:100]}...\n"
        prompt += "\n"
    
    return prompt


def create_code_assignment_prompt(predefined_themes: List[Dict[str, Any]], codes_to_assign: List[str]) -> str:
    """
    Create the second pass prompt for code assignment to predefined themes.
    
    Args:
        predefined_themes (List[Dict[str, Any]]): Themes from first pass
        codes_to_assign (List[str]): Codes to assign to themes
        
    Returns:
        str: Code assignment prompt
    """
    prompt = """Role: You are a meticulous research assistant. Your task is to classify a list of open codes into a predefined set of themes.

Input:
1. **Predefined Themes:** A list of themes with their names and descriptions.
2. **Codes to Assign:** A batch of unique open codes from student feedback.

Objective: For each code in the "Codes to Assign" list, assign it to the single most appropriate theme from the "Predefined Themes" list.

Instructions:
- Review the `theme_name` and `theme_description` for each theme to understand its scope.
- For each code, find the theme that best captures its meaning.
- Every code MUST be assigned to exactly one theme.

Required Output Format:
Your entire response MUST be a single, valid JSON array of objects.

[
  {
    "code": "The exact code string from the input",
    "assigned_theme": "The exact theme_name from the Predefined Themes list"
  }
]

---
### Data for Analysis

**1. Predefined Themes:**
"""
    
    # Add predefined themes
    for theme in predefined_themes:
        theme_name = theme.get('theme_name', '')
        theme_description = theme.get('theme_description', '')
        prompt += f"\n- **{theme_name}:** {theme_description}"
    
    prompt += "\n\n**2. Codes to Assign:**\n"
    
    # Add codes to assign
    for code in codes_to_assign:
        prompt += f"- {code}\n"
    
    return prompt


def generate_themes_two_pass(df: pd.DataFrame, enriched_codes: List[Dict[str, Any]], 
                           theme_target: int, valid_codes: set) -> Optional[Dict[str, Any]]:
    """
    Generate themes using the new two-pass strategy:
    Pass 1: Generate theme definitions only
    Pass 2: Assign all codes to the predefined themes
    
    Args:
        df (pd.DataFrame): Original coded DataFrame  
        enriched_codes (List[Dict[str, Any]]): Enriched codes with frequencies and samples
        theme_target (int): Target number of themes
        valid_codes (set): Set of valid codes for verification
        
    Returns:
        Dict[str, Any] or None: Complete themes data with assignments or None if error
    """
    try:
        print(f"üéØ Starting two-pass thematic analysis (target: {theme_target} themes)...")
        
        # PASS 1: Generate theme definitions
        print("üìù Pass 1: Generating theme definitions...")
        theme_generation_prompt = create_theme_generation_prompt(enriched_codes, theme_target)
        
        model = genai.GenerativeModel('gemini-2.5-flash')
        response = model.generate_content(theme_generation_prompt)
        
        # Parse theme generation response
        response_text = response.text.strip()
        if response_text.startswith('```json'):
            response_text = response_text[7:]
        if response_text.endswith('```'):
            response_text = response_text[:-3]
        response_text = response_text.strip()
        
        theme_definitions = json.loads(response_text)
        
        # Validate theme definitions structure
        if not isinstance(theme_definitions, dict) or 'themes' not in theme_definitions:
            raise ValueError("Theme generation response must contain 'themes' field")
        
        predefined_themes = theme_definitions['themes']
        memo = theme_definitions.get('memo', 'No memo provided')
        
        print(f"‚úÖ Pass 1 complete: Generated {len(predefined_themes)} theme definitions")
        for i, theme in enumerate(predefined_themes, 1):
            print(f"   {i}. {theme.get('theme_name', f'Theme {i}')}")
        
        # PASS 2: Assign all codes to themes
        print("üîÑ Pass 2: Assigning codes to themes...")
        
        # Get all unique codes that need assignment
        all_codes = list(valid_codes)
        
        # Process codes in batches if there are many (to avoid token limits)
        batch_size = 100  # Adjust as needed
        all_assignments = []
        
        for i in range(0, len(all_codes), batch_size):
            batch_codes = all_codes[i:i + batch_size]
            batch_num = (i // batch_size) + 1
            total_batches = math.ceil(len(all_codes) / batch_size)
            
            print(f"   Assigning batch {batch_num}/{total_batches} ({len(batch_codes)} codes)...")
            
            assignment_prompt = create_code_assignment_prompt(predefined_themes, batch_codes)
            response = model.generate_content(assignment_prompt)
            
            # Parse assignment response
            response_text = response.text.strip()
            if response_text.startswith('```json'):
                response_text = response_text[7:]
            if response_text.endswith('```'):
                response_text = response_text[:-3]
            response_text = response_text.strip()
            
            batch_assignments = json.loads(response_text)
            
            # Validate assignments
            if not isinstance(batch_assignments, list):
                raise ValueError("Code assignment response must be a JSON array")
            
            all_assignments.extend(batch_assignments)
        
        print(f"‚úÖ Pass 2 complete: Assigned {len(all_assignments)} codes")
        
        # Build final themes data structure
        themes_data = {
            'themes': [],
            'memo': memo
        }
        
        # Create theme name to assignment mapping
        theme_names = [theme['theme_name'] for theme in predefined_themes]
        theme_assignments = {theme_name: [] for theme_name in theme_names}
        
        # Group assignments by theme
        for assignment in all_assignments:
            code = assignment.get('code', '')
            assigned_theme = assignment.get('assigned_theme', '')
            
            if assigned_theme in theme_assignments:
                theme_assignments[assigned_theme].append(code)
            else:
                print(f"‚ö†Ô∏è  Warning: Code '{code}' assigned to unknown theme '{assigned_theme}'")
        
        # Build final themes with supporting codes and generate grouping rationales
        for theme_def in predefined_themes:
            theme_name = theme_def['theme_name']
            theme_description = theme_def['theme_description']
            supporting_codes = theme_assignments.get(theme_name, [])
            
            # Generate a grouping rationale based on the theme description and supporting codes
            if supporting_codes:
                # Sample first few codes to create rationale
                sample_codes = supporting_codes[:5]
                rationale = f"These codes were grouped under '{theme_name}' because they all relate to {theme_description.lower()} Key codes include: {', '.join(sample_codes[:3])}{'...' if len(supporting_codes) > 3 else ''}. This theme captures {len(supporting_codes)} distinct aspects of student feedback in this area."
            else:
                rationale = f"This theme '{theme_name}' was defined to capture {theme_description.lower()}, though no codes were assigned to it in this analysis."
            
            themes_data['themes'].append({
                'theme_name': theme_name,
                'theme_description': theme_description,
                'supporting_codes': supporting_codes,
                'grouping_rationale': rationale
            })
        
        print(f"‚úÖ Two-pass analysis complete!")
        
        # Report final statistics
        total_assigned = sum(len(theme['supporting_codes']) for theme in themes_data['themes'])
        print(f"   Total codes assigned: {total_assigned}/{len(valid_codes)}")
        
        for theme in themes_data['themes']:
            theme_name = theme['theme_name']
            code_count = len(theme['supporting_codes'])
            print(f"   - {theme_name}: {code_count} codes")
        
        return themes_data
        
    except json.JSONDecodeError as e:
        print(f"‚ùå JSON parsing error in two-pass analysis: {e}")
        return None
        
    except Exception as e:
        print(f"‚ùå Error in two-pass theme generation: {e}")
        import traceback
        traceback.print_exc()
        return None


# // ... existing code ... 